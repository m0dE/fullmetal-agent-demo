FULLMETAL_API_KEY=

#AGENT NAME
AGENT_NAME=

#MODEL INFORMATION
MODEL_NAME=
MODEL_FILE=

# -ngl N, --n-gpu-layers N number of layers to store in VRAM
NGL=28
# -n N, --n-predict N   number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)
PREDICT=512

# Path to llama.cpp folder main file (e.g: ../llama.cpp/main)
LLAMA_CPP_PATH=